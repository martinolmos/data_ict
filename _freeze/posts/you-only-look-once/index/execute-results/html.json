{
  "hash": "ff8436df425e59016c4a93065d846451",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"You Only Look Once\"\nsubtitle: \"You Only Look Once\"\nauthor: \"Martin Olmos\"\ndate: today()\ncategories: [computer-vision, deep-learning, python, yolo, ultralytics]\ntags: [computer-vision, deep-learning, python, yolo, ultralytics]\nformat: \n  html:\n    code-fold: show\nexecute: \n  eval: false\ndraft: false\n---\n\n\n::: {.content-visible when-profile=\"english\"}\n\nYou Only Look Once (YOLO) is series of open-source, real-time object detection models that can identify and classify multiple objects within an image or video frame. Unlike traditional object detection methods that rely on region proposal networks, YOLO treats object detection as a single regression problem, directly predicting bounding boxes and class probabilities from the entire image in one evaluation.\n\nThe Ultralytics YOLOv8 series supports various tasks, including object detection, pose estimation, segmentation, and classification. It is known for its speed and accuracy, making it suitable for real-time applications such as surveillance, autonomous driving, and robotics.\n\nHere is the code to run a YOLOv8 model using the Ultralytics library in Python for object detection and pose estimation:\n\n:::\n\n::: {.content-visible when-profile=\"spanish\"}\n\nYou Only Look Once (YOLO) es una serie de modelos de detección de objetos en tiempo real y de código abierto que pueden identificar y clasificar múltiples objetos dentro de una imagen o fotograma de video. A diferencia de los métodos tradicionales de detección de objetos que se basan en redes de propuestas de regiones, YOLO trata la detección de objetos como un único problema de regresión, prediciendo directamente los cuadros delimitadores y las probabilidades de clase a partir de toda la imagen en una sola evaluación.\n\nLa serie Ultralytics YOLOv8 admite varias tareas, incluida la detección de objetos, la estimación de poses, la segmentación y la clasificación. Es conocida por su velocidad y precisión, lo que la hace adecuada para aplicaciones en tiempo real como la vigilancia, la conducción autónoma y la robótica.\n\nAquí está el código para ejecutar un modelo YOLOv8 utilizando la biblioteca Ultralytics en Python para la detección de objetos y la estimación de poses:\n\n:::\n\n::: {#fcaa3591 .cell execution_count=1}\n``` {.python .cell-code}\nfrom ultralytics import YOLO\nimport cv2\nimport math \nimport torch\n\n\nCAM_INDEX = 0  # change if you have multiple cameras\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# model\nmodel = YOLO(\"yolo-Weights/yolov8n.pt\")\n\n# Optional: move model to GPU (if available)\ntry:\n    model.to(device)\nexcept Exception:\n    pass  # older ultralytics handles device per-predict call\n\n\n# start webcam\ncap = cv2.VideoCapture(CAM_INDEX)\ncap.set(3, 640)\ncap.set(4, 480)\n\n# object classes\nclassNames = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\n              \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n              \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\",\n              \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n              \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n              \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n              \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\",\n              \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n              \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n              \"teddy bear\", \"hair drier\", \"toothbrush\"\n              ]\n\n# Video output settings\noutput_path = \"output_detection.mp4\"\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = 30\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile True:\n    success, img = cap.read()\n    if not success:\n        break\n    results = model(img, stream=True, device=\"cuda\")\n\n    # coordinates\n    for r in results:\n        boxes = r.boxes\n\n        for box in boxes:\n            # bounding box\n            x1, y1, x2, y2 = box.xyxy[0]\n            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2) # convert to int values\n\n            # put box in cam\n            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n\n            # confidence\n            confidence = math.ceil((box.conf[0]*100))/100\n            print(\"Confidence --->\",confidence)\n\n            # class name\n            cls = int(box.cls[0])\n            print(\"Class name -->\", classNames[cls])\n\n            # object details\n            org = [x1, y1]\n            font = cv2.FONT_HERSHEY_SIMPLEX\n            fontScale = 1\n            color = (255, 0, 0)\n            thickness = 2\n\n            cv2.putText(img, classNames[cls], org, font, fontScale, color, thickness)\n\n    # Write frame to video file\n    out.write(img)\n    \n    cv2.imshow('Webcam', img)\n    if cv2.waitKey(1) == ord('q'):\n        break\n\ncap.release()\nout.release()  # Release the video writer\ncv2.destroyAllWindows()\nprint(f\"Video saved to {output_path}\")\n```\n:::\n\n\n![](output_detection2.mp4){width=\"640\" height=\"480\"}\n\n::: {#86d14d8a .cell execution_count=2}\n``` {.python .cell-code}\n# Realtime pose estimation with YOLOv8 (OpenCV window)\nimport cv2, torch, time\nfrom ultralytics import YOLO\n\nCAM_INDEX = 0  # change if you have multiple cameras\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel = YOLO('yolov8n-pose.pt')  # or 'yolov8s-pose.pt'\n# Optional: move model to GPU (if available)\ntry:\n    model.to(device)\nexcept Exception:\n    pass  # older ultralytics handles device per-predict call\n\ncap = cv2.VideoCapture(CAM_INDEX)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n\n# Video output settings\noutput_path = \"output_pose.mp4\"\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = 30\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nprev_t = time.time()\nwhile True:\n    ok, frame = cap.read()\n    if not ok:\n        break\n\n    results = model.predict(\n        frame,\n        device=device,\n        imgsz=640,\n        conf=0.5,\n        half=(device == 'cuda'),\n        verbose=False\n    )\n    annotated = results[0].plot()\n\n    # FPS overlay\n    now = time.time()\n    fps_val = 1 / (now - prev_t)\n    prev_t = now\n    cv2.putText(annotated, f'FPS: {fps_val:.1f} ({device})', (10, 30),\n                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n\n    # Write frame to video file\n    out.write(annotated)\n\n    cv2.imshow('YOLOv8 Pose', annotated)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncap.release()\nout.release()  # Release the video writer\ncv2.destroyAllWindows()\nprint(f\"Video saved to {output_path}\")\n```\n:::\n\n\n![](output_pose2.mp4){width=\"640\" height=\"480\"}\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}