{
  "hash": "e50f64d26b6c27cd89872994febdb129",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Ministerial Summit on the Ethics of AI in LAC\"\nsubtitle: \"Cumbre Ministerial sobre la Ética de la IA en LAC\"\nformat: \n    html: \n        code-fold: true\ndraft: true\nauthor: \"Martin Olmos\"\ndate: \"2024-11-17\"\ncategories:\n    - \"AI\"\n    - \"AWS\"\n    - \"OpenAI\"\nexecute: \n  warning: false\n  eval: false\n---\n\n::: {.content-visible when-profile=\"spanish\"}\n\nEl pasado 3 y 4 de octubre en Montevideo, Uruguay, se llevó a cabo la **2da Cumbre Ministerial y de Altas Autoridades sobre la Ética en la Inteligencia Artificial en América Latina y el Caribe**. En la misma se discutieron los desafíos y oportunidades que presenta la IA en la región, así como los principios éticos que deberían guiar su desarrollo y aplicación. \n\n[Aquí](https://foroialac.org) se puede acceder el programa completo de la cumbre y a la transmisión en vivo de las exposiciones. En este marco, se aprobó la [**Declaración de Montevideo**](https://www.gub.uy/agencia-gobierno-electronico-sociedad-informacion-conocimiento/sites/agencia-gobierno-electronico-sociedad-informacion-conocimiento/files/documentos/noticias/ESP_Declaración%20de%20Montevideo%20aprobada.pdf) y la [**Hoja de Ruta 2024-2025**](https://www.gub.uy/agencia-gobierno-electronico-sociedad-informacion-conocimiento/sites/agencia-gobierno-electronico-sociedad-informacion-conocimiento/files/documentos/noticias/ESP_Hoja%20de%20Ruta%20aprobada.pdf)\n\nPara los que no pudieron seguir el evento en vivo y no tienen tiempo para ver el video completo, aquí vamos a mostrar cómo se puede transcribir y resumir automáticamente el contenido de un video utilizando servicios de IA, en este caso las APIs de [AWS Transcribe](https://docs.aws.amazon.com/transcribe/latest/APIReference/Welcome.html) y [OpenAI](https://platform.openai.com/docs/overview).\n\n## Descarga y segmentación del video y audio de youtube\n\nPrimero es necesario descargar el video y el audio de la transmisión completa.\n\n::: {#243d0e6d .cell execution_count=1}\n``` {.python .cell-code}\nimport yt_dlp as ydl\n\nvideo_url = 'https://www.youtube.com/watch?v=mSnMpzkR2R0'\n\nydl_audio_opts = {\n    'outtmpl': 'data/cumbre_ia_montevideo.%(ext)s',\n    'format': 'bestaudio/best',\n    'postprocessors': [{\n        'key': 'FFmpegExtractAudio',\n        'preferredcodec': 'mp3',\n        'preferredquality': '192'}]\n}\nydl_video_opts = {\n    'outtmpl': 'data/cumbre_ia_montevideo.%(ext)s',\n}\n\nydl.YoutubeDL(ydl_audio_opts).download([video_url])\nydl.YoutubeDL(ydl_video_opts).download([video_url])\n```\n:::\n\n\nLuego es necesario segmentar el audio y el video en cada una de los paneles y charlas de la cumbre. A modo de muestra, vamos a extraer el panel de apertura, que va desde el comienzo al minuto 43:15.\n\n::: {#448d730c .cell execution_count=2}\n``` {.python .cell-code}\nimport pydub\nfrom moviepy.video.io.VideoFileClip import VideoFileClip\n\nduration = 43.15 * 60.00\n\naudio_segmentado = pydub.AudioSegment.from_file('../data/cumbre_ia_montevideo.mp3', duration=duration)\n\nvideo = VideoFileClip('../data/cumbre_ia_montevideo.webm')\nvideo_segmentado = video.subclip(0, duration)\n```\n:::\n\n\n## Transcripción con AWS Transcribe\n\nLa API de AWS Transcribe permite utilizar un vocabulario personalizado para mejorar la precisión de la transcripción de palabras técnicas o específicas de un dominio o nombres propios. Además, permite obtener la transcripción tanto en formato de subtítulos como en texto plano.\n\n::: {#45de9e14 .cell execution_count=3}\n``` {.python .cell-code}\nfrom __future__ import print_function\nimport time\nimport boto3\n\ntranscribe = boto3.client('transcribe', 'us-east-1')\n\njob_name = \"cumbre-ia-montevideo-apertura\"\njob_uri = \"s3://cumbre-ia-montevideo/input-audios/cumbre_ia_montevideo_apertura.mp3\"\n\ntranscribe.start_transcription_job(\n    TranscriptionJobName = job_name,\n    Media = {\n        'MediaFileUri': job_uri\n    },\n    OutputBucketName = 'cumbre-ia-montevideo',\n    OutputKey = 'output-transcriptions/', \n    LanguageCode = 'es-US', \n    Subtitles = {\n        'Formats': [\n            'vtt','srt'\n        ],\n        'OutputStartIndex': 1 \n   },\n    Settings = {\n        'ShowSpeakerLabels': True,\n        'MaxSpeakerLabels': 5,\n        'VocabularyName': 'cumbre-ia-montevideo-apertura-vocabulario'\n    }    \n)\n\nwhile True:\n    status = transcribe.get_transcription_job(TranscriptionJobName = job_name)\n    if status['TranscriptionJob']['TranscriptionJobStatus'] in ['COMPLETED', 'FAILED']:\n        break\n    print(\"Not ready yet...\")\n    time.sleep(5)\nprint(status)\n```\n:::\n\n\n## Incrustar los subtítulos en el video\n\nLuego incrustamos los subtítulos en el video.\n\n::: {#58e63834 .cell execution_count=4}\n``` {.python .cell-code}\nfrom moviepy.editor import CompositeVideoClip\nfrom moviepy.video.tools.subtitles import SubtitlesClip\nfrom moviepy.video.fx.resize import resize\nimport moviepy.editor as mp\n\n# Load subtitles from an SRT file\n# You can adjust the font size, font type, etc.\ngenerator = lambda txt: mp.TextClip(txt, font='Arial', fontsize=48, color='white')\n\n# Create the SubtitlesClip\nsubtitles = SubtitlesClip(\"../data/cumbre-ia-montevideo-apertura.srt\", generator)\n\n# Overlay the subtitles on the video\nvideo_with_subtitles = CompositeVideoClip([video_segmentado, subtitles.set_position(('center', 'bottom'))])\n\n# Write the final video file with subtitles embedded\nvideo_with_subtitles.write_videofile(\"../data/cumbre_ia_montevideo_apertura_with_subtitles.mp4\", fps=video.fps)\n```\n:::\n\n\nAquí el video de la apertura con los subtítulos incrustados:\n\n<iframe src=\"https://drive.google.com/file/d/1n8-4KT_2vK5Jo2vqSNAOBua4te5fdv4U/preview\" width=\"640\" height=\"480\" allow=\"autoplay\"></iframe>\n\n## Resumen automático con OpenAI\n\nAhora vamos a utilizar la API de OpenAI para resumir automáticamente la transcripción de las exposiciones durante la apertura de la cumbre.\n\nPrimero, es necesario hacer algo de preprocesamiento de las transcripciones.\n\n::: {#db22fe97 .cell execution_count=5}\n``` {.python .cell-code}\nimport boto3\nimport json\n\ns3_client = boto3.client('s3')\n\ntranscripcion = s3_client.get_object(Bucket='cumbre-ia-montevideo', Key='output-transcriptions/cumbre-ia-montevideo-aperura.json')['Body'].read().decode('utf-8')\n\ndata = json.loads(transcripcion)\n\n# Extraer las etiquetas de orador y las palabras\nitems = data['results']['items']\nspeaker_labels = data['results']['speaker_labels']['segments']\n\ntranscripcion_por_orador = []\n\n# Crear una estructura para mantener las intervenciones agrupadas\ncurrent_speaker = None\ncurrent_segment = []\n\nfor segment in speaker_labels:\n    speaker = segment['speaker_label']\n    start_time = float(segment['start_time'])\n    end_time = float(segment['end_time'])\n    \n    if current_speaker is None or current_speaker != speaker:\n        # Guardar la intervención anterior antes de cambiar de orador\n        if current_segment:\n            transcripcion_por_orador.append({\n                \"orador\": current_speaker,\n                \"texto\": \" \".join(current_segment)\n            })\n        # Empezar un nuevo segmento\n        current_speaker = speaker\n        current_segment = []\n\n    # Extraer las palabras dentro del rango de tiempo del segmento actual\n    for item in items:\n        if 'start_time' in item:\n            word_time = float(item['start_time'])\n            if start_time <= word_time < end_time:\n                current_segment.append(item['alternatives'][0]['content'])\n\n# Añadir la última intervención\nif current_segment:\n    transcripcion_por_orador.append({\n        \"orador\": current_speaker,\n        \"texto\": \" \".join(current_segment)\n    })\n\n\ntranscripcion_por_orador_sin_presentador = [intervencion for intervencion in transcripcion_por_orador if intervencion['orador'] != 'spk_0']\n```\n:::\n\n\nLuego, enviamos el texto de las intervenciones a OpenAI para obtener un resumen de cada exposición.\n\n::: {#be09e05d .cell execution_count=6}\n``` {.python .cell-code}\nfrom dotenv import load_dotenv\nimport openai\n\nload_dotenv()\n\nsintesis_transcripciones = []\n\nfor i in range(len(transcripcion_por_orador_sin_presentador)):\n    prompt = f\"\"\"\n    Estoy realizando una síntesis de las exposiciones en la 2da Cumbre Ministerial y de Altas Autoridades sobre la Ética en la Inteligencia Artificial. \\\n    La misma se realizó el pasado 3 y 4 de octubre en Montevideo, Uruguay. \\\n    En este caso, toca resumir la transcripción del panel {panel}, integrado por {''.join(oradores)}. \\\n    Procura corregir los errores de transcripción, sobre todo en los nombres propios. \\\n    Devuelve sólo el resumen de la transcripción, sin comentarios adicionales. \\\n    A continuación, la transcripción completa de la exposición de {oradores[i]}: \\\n    {transcripcion_por_orador_sin_presentador[i]['texto']}\n    \"\"\"\n    response = openai.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        max_tokens=1000\n    )\n    assistant_response = response.choices[0].message.content\n    sintesis_transcripciones.append({'orador': oradores[i], 'sintesis_exposicion': assistant_response})\n```\n:::\n\n\nFinalmente, mostramos los resúmenes de las exposiciones.\n\n:::\n\n::: {.content-visible when-profile=\"english\"}\n\nOn October 3 and 4 in Montevideo, Uruguay, the **2nd Ministerial and High-Level Authorities Summit on Artificial Intelligence Ethics in Latin America and the Caribbean** took place. The summit focused on discussing the challenges and opportunities that AI presents in the region, as well as the ethical principles that should guide its development and application.\n\nThe complete program of the summit and the live streaming of the sessions can be accessed [here](https://foroialac.org). Within this framework, the [**Montevideo Declaration**](https://www.gub.uy/agencia-gobierno-electronico-sociedad-informacion-conocimiento/sites/agencia-gobierno-electronico-sociedad-informacion-conocimiento/files/documentos/noticias/ESP_Declaración%20de%20Montevideo%20aprobada.pdf) and the [**2024-2025 Roadmap**](https://www.gub.uy/agencia-gobierno-electronico-sociedad-informacion-conocimiento/sites/agencia-gobierno-electronico-sociedad-informacion-conocimiento/files/documentos/noticias/ESP_Hoja%20de%20Ruta%20aprobada.pdf) were approved.\n\nFor those who could not follow the event live and do not have time to watch the entire video, here we will show how to automatically transcribe and summarize the content of a video using AI services, specifically [AWS Transcribe](https://docs.aws.amazon.com/transcribe/latest/APIReference/Welcome.html) and [OpenAI](https://platform.openai.com/docs/overview) APIs.\n\n## Downloading and Segmenting the Video and Audio from YouTube\n\nFirst, it is necessary to download the full video and audio of the broadcast.\n\n::: {#b27f2149 .cell execution_count=7}\n``` {.python .cell-code}\nimport yt_dlp as ydl\n\nvideo_url = 'https://www.youtube.com/watch?v=mSnMpzkR2R0'\n\nydl_audio_opts = {\n    'outtmpl': 'data/cumbre_ia_montevideo.%(ext)s',\n    'format': 'bestaudio/best',\n    'postprocessors': [{\n        'key': 'FFmpegExtractAudio',\n        'preferredcodec': 'mp3',\n        'preferredquality': '192'}]\n}\nydl_video_opts = {\n    'outtmpl': 'data/cumbre_ia_montevideo.%(ext)s',\n}\n\nydl.YoutubeDL(ydl_audio_opts).download([video_url])\nydl.YoutubeDL(ydl_video_opts).download([video_url])\n```\n:::\n\n\nThen, it is necessary to segment the audio and video into each of the panels and talks from the summit. As an example, we will extract the opening panel, which runs from the beginning to minute 43:15.\n\n::: {#0fd21685 .cell execution_count=8}\n``` {.python .cell-code}\nimport pydub\nfrom moviepy.video.io.VideoFileClip import VideoFileClip\n\nduration = 43.15 * 60.00\n\naudio_segmentado = pydub.AudioSegment.from_file('../data/cumbre_ia_montevideo.mp3', duration=duration)\n\nvideo = VideoFileClip('../data/cumbre_ia_montevideo.webm')\nvideo_segmentado = video.subclip(0, duration)\n```\n:::\n\n\n## Transcription with AWS Transcribe\n\nThe AWS Transcribe API allows the use of a customized vocabulary to improve the accuracy of transcribing technical terms, domain-specific words, or proper names. Additionally, it provides the transcript in both subtitle format and plain text.\n\n::: {#1e17f860 .cell execution_count=9}\n``` {.python .cell-code}\nfrom __future__ import print_function\nimport time\nimport boto3\n\ntranscribe = boto3.client('transcribe', 'us-east-1')\n\njob_name = \"cumbre-ia-montevideo-apertura\"\njob_uri = \"s3://cumbre-ia-montevideo/input-audios/cumbre_ia_montevideo_apertura.mp3\"\n\ntranscribe.start_transcription_job(\n    TranscriptionJobName = job_name,\n    Media = {\n        'MediaFileUri': job_uri\n    },\n    OutputBucketName = 'cumbre-ia-montevideo',\n    OutputKey = 'output-transcriptions/', \n    LanguageCode = 'es-US', \n    Subtitles = {\n        'Formats': [\n            'vtt','srt'\n        ],\n        'OutputStartIndex': 1 \n   },\n    Settings = {\n        'ShowSpeakerLabels': True,\n        'MaxSpeakerLabels': 5,\n        'VocabularyName': 'cumbre-ia-montevideo-apertura-vocabulario'\n    }    \n)\n\nwhile True:\n    status = transcribe.get_transcription_job(TranscriptionJobName = job_name)\n    if status['TranscriptionJob']['TranscriptionJobStatus'] in ['COMPLETED', 'FAILED']:\n        break\n    print(\"Not ready yet...\")\n    time.sleep(5)\nprint(status)\n```\n:::\n\n\n## Embedding the Subtitles into the Video\n\nWe then embed the subtitles into the video.\n\n::: {#df33333e .cell execution_count=10}\n``` {.python .cell-code}\nfrom moviepy.editor import CompositeVideoClip\nfrom moviepy.video.tools.subtitles import SubtitlesClip\nfrom moviepy.video.fx.resize import resize\nimport moviepy.editor as mp\n\n# Load subtitles from an SRT file\n# You can adjust the font size, font type, etc.\ngenerator = lambda txt: mp.TextClip(txt, font='Arial', fontsize=48, color='white')\n\n# Create the SubtitlesClip\nsubtitles = SubtitlesClip(\"../data/cumbre-ia-montevideo-apertura.srt\", generator)\n\n# Overlay the subtitles on the video\nvideo_with_subtitles = CompositeVideoClip([video_segmentado, subtitles.set_position(('center', 'bottom'))])\n\n# Write the final video file with subtitles embedded\nvideo_with_subtitles.write_videofile(\"../data/cumbre_ia_montevideo_apertura_with_subtitles.mp4\", fps=video.fps)\n```\n:::\n\n\nHere is the opening video with the embedded subtitles:\n\n<iframe src=\"https://drive.google.com/file/d/1n8-4KT_2vK5Jo2vqSNAOBua4te5fdv4U/preview\" width=\"640\" height=\"480\" allow=\"autoplay\"></iframe>\n\n## Automatic Summary with OpenAI\n\nNow we will use the OpenAI API to automatically summarize the transcriptions of the speeches during the summit's opening.\n\nFirst, some preprocessing of the transcriptions is required.\n\n::: {#de74b3e5 .cell execution_count=11}\n``` {.python .cell-code}\nimport boto3\nimport json\n\ns3_client = boto3.client('s3')\n\ntranscripcion = s3_client.get_object(Bucket='cumbre-ia-montevideo', Key='output-transcriptions/cumbre-ia-montevideo-aperura.json')['Body'].read().decode('utf-8')\n\ndata = json.loads(transcripcion)\n\n# Extraer las etiquetas de orador y las palabras\nitems = data['results']['items']\nspeaker_labels = data['results']['speaker_labels']['segments']\n\ntranscripcion_por_orador = []\n\n# Crear una estructura para mantener las intervenciones agrupadas\ncurrent_speaker = None\ncurrent_segment = []\n\nfor segment in speaker_labels:\n    speaker = segment['speaker_label']\n    start_time = float(segment['start_time'])\n    end_time = float(segment['end_time'])\n    \n    if current_speaker is None or current_speaker != speaker:\n        # Guardar la intervención anterior antes de cambiar de orador\n        if current_segment:\n            transcripcion_por_orador.append({\n                \"orador\": current_speaker,\n                \"texto\": \" \".join(current_segment)\n            })\n        # Empezar un nuevo segmento\n        current_speaker = speaker\n        current_segment = []\n\n    # Extraer las palabras dentro del rango de tiempo del segmento actual\n    for item in items:\n        if 'start_time' in item:\n            word_time = float(item['start_time'])\n            if start_time <= word_time < end_time:\n                current_segment.append(item['alternatives'][0]['content'])\n\n# Añadir la última intervención\nif current_segment:\n    transcripcion_por_orador.append({\n        \"orador\": current_speaker,\n        \"texto\": \" \".join(current_segment)\n    })\n\n\ntranscripcion_por_orador_sin_presentador = [intervencion for intervencion in transcripcion_por_orador if intervencion['orador'] != 'spk_0']\n```\n:::\n\n\nThen, we send the text of the speeches to OpenAI to obtain a summary of each presentation.\n\n::: {#f09ac1eb .cell execution_count=12}\n``` {.python .cell-code}\nfrom dotenv import load_dotenv\nimport openai\n\nload_dotenv()\n\nsintesis_transcripciones = []\n\nfor i in range(len(transcripcion_por_orador_sin_presentador)):\n    prompt = f\"\"\"\n    Estoy realizando una síntesis de las exposiciones en la 2da Cumbre Ministerial y de Altas Autoridades sobre la Ética en la Inteligencia Artificial. \\\n    La misma se realizó el pasado 3 y 4 de octubre en Montevideo, Uruguay. \\\n    En este caso, toca resumir la transcripción del panel {panel}, integrado por {''.join(oradores)}. \\\n    Procura corregir los errores de transcripción, sobre todo en los nombres propios. \\\n    Devuelve sólo el resumen de la transcripción, sin comentarios adicionales. \\\n    A continuación, la transcripción completa de la exposición de {oradores[i]}: \\\n    {transcripcion_por_orador_sin_presentador[i]['texto']}\n    \"\"\"\n    response = openai.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        max_tokens=1000\n    )\n    assistant_response = response.choices[0].message.content\n    sintesis_transcripciones.append({'orador': oradores[i], 'sintesis_exposicion': assistant_response})\n```\n:::\n\n\nFinally, we display the summaries of the speeches.\n\n:::\n\n::: {#8b251c37 .cell execution_count=13}\n``` {.python .cell-code}\nfrom IPython.display import display, Markdown\n\nfor sintesis in sintesis_transcripciones:\n    display(Markdown(f\"### {sintesis['orador']}:\\n\\n{sintesis['sintesis_exposicion']}\"))\n```\n:::\n\n\n### Beatriz Argimón, Vicepresidenta de la República Oriental del Uruguay:\n\nBeatriz Argimón, Vicepresidenta de la República Oriental del Uruguay, expresó su entusiasmo por participar en la apertura de la 2da Cumbre Ministerial sobre la Ética en la Inteligencia Artificial. Destacó la importancia de los debates éticos y el protagonismo que deben asumir aquellos con responsabilidades, sobre todo en un país con alta adhesión democrática como Uruguay. Señaló la relevancia de proteger los derechos humanos y las democracias frente a los cambios vertiginosos de la inteligencia artificial.\n\nArgimón enfatizó la necesidad de un enfoque regional unido en América Latina y el Caribe para avanzar en el ámbito global y subrayó la importancia de enfrentar los desafíos con responsabilidad ética. Expresó su orgullo por las políticas de Estado de Uruguay, independientes del partido político gobernante, y elogió el papel de AGESIC y Hebert Paguas en la creación de una conciencia estratégica en el ámbito público.\n\nFinalmente, habló sobre la responsabilidad democrática de informar y educar a la ciudadanía sobre estos nuevos tiempos y agradeció a los organizadores de la cumbre por promover el entendimiento de que estos tiempos, aunque desafiantes, también son esperanzadores.\n\n### Christian Asinelli, Vicepresidente Corporativo de Programación Estratégica, CAF -banco de desarrollo de América Latina y el Caribe-:\n\nEn la 2da Cumbre Ministerial y de Altas Autoridades sobre la Ética en la Inteligencia Artificial, celebrada en Montevideo, Uruguay, el discurso de apertura de Christian Asinelli, Vicepresidente Corporativo de Programación Estratégica de CAF -banco de desarrollo de América Latina y el Caribe-, abordó varios puntos clave:\n\nChristian Asinelli resaltó la importancia de abordar los problemas globales con soluciones regionales, especialmente en el ámbito de la inteligencia artificial (IA) y otras áreas como la energía y la alimentación. Destacó el esfuerzo conjunto con la UNESCO para implementar políticas públicas de IA y mencionó la anterior Cumbre de Ética de la Inteligencia Artificial en Chile, donde se lanzó la Declaración de Santiago. Asinelli expresó la expectativa de que la Declaración de Montevideo continúe avanzando en capacidades regionales.\n\nAdemás, Asinelli enfatizó la necesidad de una transición justa e inclusiva en América Latina y el Caribe, considerando las capacidades fiscales y la pobreza en la región, y subrayó los riesgos de la IA, incluyendo derechos humanos, transparencia y democracia. Destacó la colaboración con diferentes organizaciones y la intención de crear una hoja de ruta con un enfoque holístico que sitúe al ser humano en el centro de las políticas públicas de IA. También mencionó la importancia de los espacios de diálogo y reflexión durante la cumbre para promover una IA al servicio de la comunidad. \n\nEn conclusión, Asinelli agradeció los esfuerzos conjuntos del gobierno de Uruguay, la UNESCO y todos los involucrados en la organización de esta cumbre, proyectando que será un evento significativo en el desarrollo de la IA ética en la región.\n\n### Gabriela Ramos, Subdirectora General de Ciencias Sociales y Humanas, UNESCO:\n\nEn su intervención, Gabriela Ramos, Subdirectora General de Ciencias Sociales y Humanas de la UNESCO, agradeció al Gobierno de Uruguay y a los participantes de la cumbre por la oportunidad de discutir temas relevantes sobre la ética en la inteligencia artificial. Resaltó la importancia de América Latina en definir su propio destino tecnológico y enfatizó que el proceso involucra no solo aspectos tecnológicos, sino sociales, económicos y de visión para el desarrollo sostenible.\n\nMencionó que el proceso iniciado con el Consenso de Santiago ha sido destacado a nivel internacional y subrayó la necesidad de seguir avanzando con nuevas etapas, como las planificadas para la República Dominicana. Ramos destacó los retos y logros de los países en sus hojas de ruta (RAM), señalando el éxito de Uruguay en diversas áreas como la protección de datos y la energía renovable.\n\nRamos enfatizó el crecimiento significativo de la inversión global en inteligencia artificial y la necesidad de que las tecnologías sirvan para resolver problemas humanos. Asimismo, instó a los países de América Latina a incrementar sus inversiones en investigación y desarrollo, sugiriendo que un aumento del PIB dedicado a este sector podría impulsar el crecimiento económico y social.\n\nFinalmente, subrayó la importancia de la ética en el desarrollo tecnológico y el papel de las competencias humanas, proponiendo una educación que fomente el pensamiento crítico y la inclusión de humanidades en programas tecnológicos. Concluyó reiterando el compromiso de la UNESCO de trabajar conjuntamente con las naciones de la región para aprovechar la inteligencia artificial de manera inclusiva y beneficiosa.\n\n### Hebert Paguas, Director Ejecutivo de AGESIC:\n\nHebert Paguas, Director Ejecutivo de AGESIC, comenzó su intervención en la Cumbre reflexionando sobre el reto de abordar el ritmo acelerado de los cambios tecnológicos, especialmente en comparación con la lentitud de los procesos legislativos tradicionales. Subrayó que aunque la inteligencia artificial (IA) se definió por primera vez en 1956, el debate significativo sobre su influencia apenas tomó auge en 2022 con la aparición de la inteligencia artificial generativa, que simula una conversación humana. Paguas expresó su deseo de que la tecnología no llegue a suplantar la esencia humana, diferenciando a los humanos como \"Homo Viator\", seres en constante viaje y búsqueda de madurez.\n\nAdemás, destacó la importancia de la colaboración regional e internacional para enfrentar los desafíos que surgen en el entorno digital, donde los límites territoriales de la legislación se vuelven obsoletos. Citó la necesidad de coordinación internacional similar a la que ocurre en el mundo físico con convenios como los de extradición, ahora trasladados al ámbito digital.\n\nPaguas también mencionó el Pacto Global Digital y enfatizó tanto en los beneficios potenciales de la tecnología como en los riesgos aún desconocidos que conlleva. Agradeció a organizaciones como la UNESCO y la CAF por su apoyo logístico y esfuerzo colaborativo en el evento, y concluyó destacando la importancia de que Uruguay se posicione como un polo de innovación, resaltando el rol necesario del sector privado y la preparación del sector público para enfrentar los retos presentes y futuros.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}